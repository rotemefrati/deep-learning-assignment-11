{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN98i96VgI1RPiDcxyhIDEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rotemefrati/deep-learning-assignment-11/blob/main/deep_learning_assignment_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Student Name: Rotem Efrati***"
      ],
      "metadata": {
        "id": "7LzbKtWIRMjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "5GCyrV3oen9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.a**"
      ],
      "metadata": {
        "id": "vHlcn-hWfreL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_as(A, B):\n",
        "  \"\"\"Mimics functionality of torch.Tensor.expand_as.\n",
        "\n",
        "  Args:\n",
        "    PyTorch tensors A, B.\n",
        "\n",
        "  Returns:\n",
        "    PyTorch tensor C: The broadcast of A to the size of B.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Tensor A is not broadcastable to tensor B.\n",
        "  \"\"\"\n",
        "  # Check if A is broadcastable to B by broadcast rules.\n",
        "  if not A.dim() <= B.dim():\n",
        "    raise ValueError(\"Cannot broadcast A to B as the dimension of A is larger than the dimension of B\")\n",
        "\n",
        "  i, j = A.dim() - 1, B.dim() - 1\n",
        "  while i >= 0:\n",
        "    if A.size(i) != B.size(j) and A.size(i) != 1:\n",
        "      raise ValueError(\"Tensor A is not broadcastable to B\")  # Because of broadcast rules.\n",
        "    i, j = i - 1, j - 1\n",
        "\n",
        "  # If we got so far, A is broadcastable to B.\n",
        "  C = A.clone()\n",
        "\n",
        "  # If A has less dimensions than B, add dimensions to A.\n",
        "  if C.dim() != B.dim():  # Then dim A is smaller than dim B.\n",
        "    dims_to_add = B.dim() - C.dim()\n",
        "    for _ in range(dims_to_add):\n",
        "      C.unsqueeze_(0)  # We add sized-1 dimensions to the start of A, until it equals dim B.\n",
        "\n",
        "  # Now A and B have the same amount of dimensions.\n",
        "  i = C.dim() - 1\n",
        "  while i >= 0:\n",
        "    if C.size(i) != B.size(i):  # Then A's size at this dimension is 1.\n",
        "      C = torch.cat([C] * B.size(i), i)  # Duplicate A along this dimension.\n",
        "    i -= 1\n",
        "\n",
        "  return C"
      ],
      "metadata": {
        "id": "z0jemo5Peq9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.b**"
      ],
      "metadata": {
        "id": "st9K_nybWtp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def are_broadcastable_together(A, B):\n",
        "  \"\"\"Checks if tensors A and B are broadcastable together.\n",
        "\n",
        "  Args:\n",
        "    PyTorch tensors A, B.\n",
        "\n",
        "  Returns:\n",
        "    False, if tensors are not broadcastable.\n",
        "    (True, torch.Size) containing the broadcasted shape, if tensors are broadcastable.\n",
        "  \"\"\"\n",
        "  # Check if A and B are broadcastable by the general broadcast rules.\n",
        "  k = min(A.dim(), B.dim()) - 1  # To iterate over the tensor with the smaller dimension.\n",
        "  i, j = A.dim() - 1, B.dim() - 1\n",
        "  while k >= 0:\n",
        "    if A.size(i) != B.size(j) and A.size(i) != 1 and B.size(j) != 1:\n",
        "      return False\n",
        "    k, i, j = k-1, i-1, j-1\n",
        "\n",
        "  # If we got so far, A and B are broadcastable.\n",
        "  # Create a new list that will be containing the broadcasted shape.\n",
        "  broadcasted_size = []\n",
        "\n",
        "  k = min(A.dim(), B.dim()) - 1\n",
        "  i, j = A.dim() - 1, B.dim() - 1\n",
        "  # Take the larger dimension for the result.\n",
        "  while k >= 0:\n",
        "    if A.size(i) < B.size(j):\n",
        "      broadcasted_size.insert(0, B.size(j))\n",
        "    else:\n",
        "      broadcasted_size.insert(0, A.size(i))\n",
        "    k, i, j = k-1, i-1, j-1\n",
        "\n",
        "  # Handle the remaining dimensions.\n",
        "  for _ in range(abs(A.dim() - B.dim())):\n",
        "    if A.dim() < B.dim():\n",
        "      broadcasted_size.insert(0, B.size(j))\n",
        "      j -= 1\n",
        "    else:\n",
        "      broadcasted_size.insert(0, A.size(i))\n",
        "      i -= 1\n",
        "\n",
        "  return True, torch.Size(broadcasted_size)"
      ],
      "metadata": {
        "id": "hQDbRE9HBh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.c**"
      ],
      "metadata": {
        "id": "vswwDdv8KAPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def broadcast_tensors(A, B):\n",
        "  \"\"\"Mimics functionality of torch.broadcast_tensors.\n",
        "\n",
        "  Args:\n",
        "    PyTorch tensors A, B.\n",
        "\n",
        "  Returns:\n",
        "    Two new tensors, which are the broadcast results of A and B.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If tensors are not broadcastable together.\n",
        "  \"\"\"\n",
        "  # Check if tensors A, B are broadcastable together.\n",
        "  result = are_broadcastable_together(A, B)\n",
        "\n",
        "  if result is False:\n",
        "    raise ValueError(\"The tensors are not broadcastable together\")\n",
        "  else:\n",
        "    broadcastable, broadcasted_size = result  # Unpack the result.\n",
        "\n",
        "  # Create a new empty tensor with the broadcasted size.\n",
        "  broadcasted_tensor = torch.empty(broadcasted_size)\n",
        "  # Broadcast A and B to their new common size and return them.\n",
        "  return expand_as(A, broadcasted_tensor), expand_as(B, broadcasted_tensor)"
      ],
      "metadata": {
        "id": "kjGnw-bnKE0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.d**"
      ],
      "metadata": {
        "id": "NmkDMPP9DspE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A1 = torch.tensor([1])\n",
        "B1 = torch.tensor([1, 2, 3, 4])\n",
        "print(\"A1:\", A1, \"B1:\", B1, sep='\\n')\n",
        "print(\"A1 is broadcastable to B1:\")\n",
        "print(expand_as(A1, B1), A1.expand_as(B1), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sESt4sRGfrj2",
        "outputId": "dcbcced0-cc37-421e-868b-87bc5b3602d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1:\n",
            "tensor([1])\n",
            "B1:\n",
            "tensor([1, 2, 3, 4])\n",
            "A1 is broadcastable to B1:\n",
            "tensor([1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A2 = torch.arange(3)\n",
        "B2 = torch.arange(9).reshape(3,3)\n",
        "print(\"A2:\", A2, \"B2:\", B2, sep='\\n')\n",
        "print(\"A2 is broadcastable to B2:\")\n",
        "print(expand_as(A2, B2), A2.expand_as(B2), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1daRitDft-Q",
        "outputId": "c0fda9ce-21d6-407e-9196-46e83578b9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A2:\n",
            "tensor([0, 1, 2])\n",
            "B2:\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "A2 is broadcastable to B2:\n",
            "tensor([[0, 1, 2],\n",
            "        [0, 1, 2],\n",
            "        [0, 1, 2]])\n",
            "tensor([[0, 1, 2],\n",
            "        [0, 1, 2],\n",
            "        [0, 1, 2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A3 = torch.arange(3).reshape(3,1)\n",
        "B3 = torch.arange(4).reshape(1,4)\n",
        "print(\"A3:\", A3, \"B3:\", B3, sep='\\n')\n",
        "print(\"A3 and B3 are broadcastable together:\")\n",
        "myA3, myB3 = broadcast_tensors(A3, B3)\n",
        "pyA3, pyB3 = torch.broadcast_tensors(A3, B3)\n",
        "print(myA3, pyA3, myB3, pyB3, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH0omoLygDur",
        "outputId": "173f2e86-3cd5-4c25-8d47-fd54602fd393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A3:\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]])\n",
            "B3:\n",
            "tensor([[0, 1, 2, 3]])\n",
            "A3 and B3 are broadcastable together:\n",
            "tensor([[0, 0, 0, 0],\n",
            "        [1, 1, 1, 1],\n",
            "        [2, 2, 2, 2]])\n",
            "tensor([[0, 0, 0, 0],\n",
            "        [1, 1, 1, 1],\n",
            "        [2, 2, 2, 2]])\n",
            "tensor([[0, 1, 2, 3],\n",
            "        [0, 1, 2, 3],\n",
            "        [0, 1, 2, 3]])\n",
            "tensor([[0, 1, 2, 3],\n",
            "        [0, 1, 2, 3],\n",
            "        [0, 1, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A4 = torch.arange(6).reshape(3,2)\n",
        "B4 = torch.arange(5).reshape(1,5)\n",
        "print(\"A4:\", A4, \"B4:\", B4, sep='\\n')\n",
        "print(\"A4 and B4 are not broadcastable together:\")\n",
        "try:\n",
        "  broadcast_tensors(A4, B4)\n",
        "except Exception as e:\n",
        "  print(f\"My function error: {e}\")\n",
        "try:\n",
        "  torch.broadcast_tensors(A4, B4)\n",
        "except Exception as e:\n",
        "  print(f\"PyTorch function error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFCBhpXzgGJY",
        "outputId": "d1ec1e91-1b79-4c96-f541-d1a61e22dcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A4:\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n",
            "B4:\n",
            "tensor([[0, 1, 2, 3, 4]])\n",
            "A4 and B4 are not broadcastable together:\n",
            "My function error: The tensors are not broadcastable together\n",
            "PyTorch function error: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A5 = torch.arange(24).reshape(2,3,4)\n",
        "B5 = torch.tensor([0,1,2,3])\n",
        "print(\"A5:\", A5, \"B5:\", B5, sep='\\n')\n",
        "print(\"A5 is not broadcastable to B5, but B5 is broadcastable to A5:\")\n",
        "try:\n",
        "  A5.expand_as(B5)\n",
        "except Exception as e:\n",
        "  print(f\"PyTorch function error: {e}\")\n",
        "try:\n",
        "  expand_as(A5, B5)\n",
        "except Exception as e:\n",
        "  print(f\"My function error: {e}\")\n",
        "print(B5.expand_as(A5), expand_as(B5, A5), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjY3cGEDgJj7",
        "outputId": "0bf52ea4-e473-4b94-db7b-72cdd0f7e650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A5:\n",
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "B5:\n",
            "tensor([0, 1, 2, 3])\n",
            "A5 is not broadcastable to B5, but B5 is broadcastable to A5:\n",
            "PyTorch function error: expand(torch.LongTensor{[2, 3, 4]}, size=[4]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)\n",
            "My function error: Cannot broadcast A to B as the dimension of A is larger than the dimension of B\n",
            "tensor([[[0, 1, 2, 3],\n",
            "         [0, 1, 2, 3],\n",
            "         [0, 1, 2, 3]],\n",
            "\n",
            "        [[0, 1, 2, 3],\n",
            "         [0, 1, 2, 3],\n",
            "         [0, 1, 2, 3]]])\n",
            "tensor([[[0, 1, 2, 3],\n",
            "         [0, 1, 2, 3],\n",
            "         [0, 1, 2, 3]],\n",
            "\n",
            "        [[0, 1, 2, 3],\n",
            "         [0, 1, 2, 3],\n",
            "         [0, 1, 2, 3]]])\n"
          ]
        }
      ]
    }
  ]
}